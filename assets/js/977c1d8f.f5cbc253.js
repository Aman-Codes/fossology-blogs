(self.webpackChunkfossology_blogs=self.webpackChunkfossology_blogs||[]).push([[654],{3905:function(e,t,n){"use strict";n.d(t,{Zo:function(){return c},kt:function(){return h}});var o=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,i=function(e,t){if(null==e)return{};var n,o,i={},a=Object.keys(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=o.createContext({}),d=function(e){var t=o.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=d(e.components);return o.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},u=o.forwardRef((function(e,t){var n=e.components,i=e.mdxType,a=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=d(n),h=i,g=u["".concat(l,".").concat(h)]||u[h]||p[h]||a;return n?o.createElement(g,r(r({ref:t},c),{},{components:n})):o.createElement(g,r({ref:t},c))}));function h(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=n.length,r=new Array(a);r[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,r[1]=s;for(var d=2;d<a;d++)r[d]=n[d];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}u.displayName="MDXCreateElement"},1061:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return r},metadata:function(){return s},toc:function(){return l},default:function(){return c}});var o=n(2122),i=n(9756),a=(n(7294),n(3905)),r={},s={unversionedId:"docs/Migration-to-UTF-8-DB",id:"docs/Migration-to-UTF-8-DB",isDocsHomePage:!1,title:"Introduction to problem",description:"When FOSSology was launched, the default character-set encoding for Postgres database was the evil SQLASCII. The SQLASCII represents bytes between 0-127 according to ASCII standards but stores the rest of 128-255 bytes as uninterpreted data. Which means you can store any binary value in it and Postgres will not complaint.",source:"@site/docs/docs/Migration-to-UTF-8-DB.md",sourceDirName:"docs",slug:"/docs/Migration-to-UTF-8-DB",permalink:"/fossology-blogs/docs/docs/Migration-to-UTF-8-DB",editUrl:"https://github.com/Aman-Codes/fossology-blogs/edit/master/docs/docs/Migration-to-UTF-8-DB.md",version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Maintenance-Agent",permalink:"/fossology-blogs/docs/docs/Maintenance-Agent"},next:{title:"Mimetype-Agent",permalink:"/fossology-blogs/docs/docs/Mimetype-Agent"}},l=[{value:"Fixing pg_dump (fast)",id:"fixing-pg_dump-fast",children:[{value:"Steps for migration",id:"steps-for-migration",children:[]}]},{value:"Fixing existing DB (slow)",id:"fixing-existing-db-slow",children:[{value:"Steps for migration",id:"steps-for-migration-1",children:[]},{value:"Related issues",id:"related-issues",children:[]}]}],d={toc:l};function c(e){var t=e.components,n=(0,i.Z)(e,["components"]);return(0,a.kt)("wrapper",(0,o.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"When FOSSology was launched, the default character-set encoding for Postgres database was the evil ",(0,a.kt)("inlineCode",{parentName:"p"},"SQL_ASCII"),". The ",(0,a.kt)("inlineCode",{parentName:"p"},"SQL_ASCII")," represents bytes between 0-127 according to ASCII standards but stores the rest of 128-255 bytes as uninterpreted data. Which means you can store any binary value in it and Postgres will not complaint."),(0,a.kt)("p",null,"This caused multiple issues with copyright agent. Since copyright agent compares file content using RegEx and store anything it finds to DB (which defaults to ",(0,a.kt)("inlineCode",{parentName:"p"},"SQL_ASCII"),") and this text can contain unicode characters or even binary which is not encoded anywhere. This also causes issues while generating reports. But with newer versions of PostgreSQL database, the default encoding changed to ",(0,a.kt)("inlineCode",{parentName:"p"},"UTF-8"),"."),(0,a.kt)("h1",{id:"need-of-external-migration-tool"},"Need of external migration tool"),(0,a.kt)("p",null,"Since old DB already contains non-encoded unicode characters plus binary data, it become impossible to simply use ",(0,a.kt)("inlineCode",{parentName:"p"},"pg_dump")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"psql")," method to migrate data from old DB with ",(0,a.kt)("inlineCode",{parentName:"p"},"SQL_ASCII")," encoding to new DB with ",(0,a.kt)("inlineCode",{parentName:"p"},"UTF-8")," encoding."),(0,a.kt)("p",null,"To fix this, the binary strings have to be removed and the unicode data needs to be recoded."),(0,a.kt)("h1",{id:"migration-solutions"},"Migration solutions"),(0,a.kt)("h2",{id:"fixing-pg_dump-fast"},"Fixing pg_dump (fast)"),(0,a.kt)("p",null,"One of the solution is to run additional program provided by ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/fossology/fossology/pull/1557"},"PR #1557"),". The program uses ",(0,a.kt)("inlineCode",{parentName:"p"},"libicu")," to do the magic. It works on dump created by ",(0,a.kt)("inlineCode",{parentName:"p"},"pg_dump")," and is the recommended way for large DB as the read/write happens directly on filesystem."),(0,a.kt)("h3",{id:"steps-for-migration"},"Steps for migration"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Run the ",(0,a.kt)("inlineCode",{parentName:"li"},"pg_dump")," to get the sql file (do not use any format other than plain)"),(0,a.kt)("li",{parentName:"ol"},"Compile the tool")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"$ make -C src/copyright fo_unicode_clean\n")),(0,a.kt)("ol",{start:3},(0,a.kt)("li",{parentName:"ol"},"Run the tool")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"$ ./src/copyright/fo_unicode_clean -i <pg_dump.sql> -o <encoded_dump.sql>\n")),(0,a.kt)("ol",{start:4},(0,a.kt)("li",{parentName:"ol"},"Change the encoding in SQL dump (",(0,a.kt)("inlineCode",{parentName:"li"},"<encoded_dump.sql>"),")",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Since old DB was with ",(0,a.kt)("inlineCode",{parentName:"li"},"SQL_ASCII")," encoding, the dump generated will have ",(0,a.kt)("inlineCode",{parentName:"li"},"SQL_ASCII")," as the encoding for the table."),(0,a.kt)("li",{parentName:"ul"},"Either edit the file using tools like ",(0,a.kt)("inlineCode",{parentName:"li"},"sed")," or ",(0,a.kt)("inlineCode",{parentName:"li"},"awk"),". If the dump file is huge, this might be a tricky job."),(0,a.kt)("li",{parentName:"ul"},"Or manually create the DB in new host (make sure the new DB have exactly the same name as old DB otherwise the dump will create a new DB and restore data to it) with ",(0,a.kt)("inlineCode",{parentName:"li"},"UTF-8")," encoding. While restoring the DB, this will result in an error but unless ",(0,a.kt)("inlineCode",{parentName:"li"},"ON_ERROR_STOP")," is set, you can ignore it."))),(0,a.kt)("li",{parentName:"ol"},"Restore the data using ",(0,a.kt)("inlineCode",{parentName:"li"},"psql --file=<encoded_dump.sql>"))),(0,a.kt)("h2",{id:"fixing-existing-db-slow"},"Fixing existing DB (slow)"),(0,a.kt)("p",null,"With the help of ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/fossology/fossology/pull/1651"},"PR #1651"),", a new script was added which can do recoding of data in current DB. The script directly reads the data from DB, fix the encoding, and write it back to the DB. This involves interaction in DB, thus this method is slow. This method is recommended if you have small DB or do not want to use ",(0,a.kt)("inlineCode",{parentName:"p"},"pg_dump"),"."),(0,a.kt)("h3",{id:"steps-for-migration-1"},"Steps for migration"),(0,a.kt)("p",null,"The script should work whenever you run ",(0,a.kt)("inlineCode",{parentName:"p"},"fo-postinstall"),". But you will receive a warning if the total rows are more than 500,000."),(0,a.kt)("p",null,"To force the encoding, you can either"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Run the script from the path provided in the warning."),(0,a.kt)("li",{parentName:"ol"},"Run the ",(0,a.kt)("inlineCode",{parentName:"li"},"fo-postinstall")," with ",(0,a.kt)("inlineCode",{parentName:"li"},"--force-encode")," flag.")),(0,a.kt)("p",null,"Once the script finishes, it also changes the encoding of database to ",(0,a.kt)("inlineCode",{parentName:"p"},"UTF-8"),"."),(0,a.kt)("h1",{id:"changes-done-in-copyright-agent"},"Changes done in copyright agent"),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"libicu")," to the rescue. As with the migration tool ",(0,a.kt)("inlineCode",{parentName:"p"},"fo_unicode_clean"),", the copyright (and sister agents) and ojo agent now filter the text before writing the text to DB. First change to clean non UTF-8 text was done with ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/fossology/fossology/pull/1528"},"PR #1528")," and later the function was moved to FOSSology library in ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/fossology/fossology/pull/1651/files#diff-319fae1eeaaf8f40905e80e30abca806R43"},"PR #1651")," under the function named ",(0,a.kt)("inlineCode",{parentName:"p"},"recodeToUnicode()"),"."),(0,a.kt)("p",null,"These changes will help prevent non UTF-8 text getting into DB with copyright (and sister agents) and ojo. Also, if the DB encoding is already UTF-8, it will eliminate the chance of DB rejecting the data since it is not UTF-8 compatible reducing data loss."),(0,a.kt)("h3",{id:"related-issues"},"Related issues"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://github.com/fossology/fossology/issues/1492"},"https://github.com/fossology/fossology/issues/1492")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://github.com/fossology/fossology/issues/1203"},"https://github.com/fossology/fossology/issues/1203"))))}c.isMDXComponent=!0}}]);